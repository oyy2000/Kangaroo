{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:14<00:00,  7.38s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-top_p_0.5_temp_0.1/safety/jailbreak.json\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 1\n",
      "100%|███████████████████████████████████| 1400/1400 [00:00<00:00, 270550.39it/s]\n",
      "#Mean wall time:  6.475097922597612\n",
      "#Mean accepted tokens:  2.0140632749484455\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m evaluation.inference_kangaroo_typical_sampling --task \"safety\" --subtask \"jailbreak\" --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --temperature 0.1 --steps 6  --bench-name \"Kangaroo\" --dtype \"float16\" --do_sample \"top_p\" --max-new-tokens 1024 --hyper_p 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|█████████         | 1/2 [00:10<00:10, 10.96s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m evaluation.inference_kangaroo_typical_sampling --task \"safety\" --subtask \"jailbreak\" --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --temperature 0.4 --steps 6  --bench-name \"Kangaroo\" --dtype \"float16\" --do_sample \"top_p\" --max-new-tokens 1024 --hyper_p 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:14<00:00,  7.17s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-top_p_0.5_temp_0.5/safety/jailbreak.json\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 1\n",
      "100%|█████████████████████████████████████| 1400/1400 [2:38:03<00:00,  6.77s/it]\n",
      "#Mean wall time:  6.615993694067002\n",
      "#Mean accepted tokens:  2.03417147727713\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m evaluation.inference_kangaroo_typical_sampling --task \"safety\" --subtask \"jailbreak\" --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --temperature 0.6 --steps 6  --bench-name \"Kangaroo\" --dtype \"float16\" --do_sample \"top_p\" --max-new-tokens 1024 --hyper_p 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m evaluation.inference_kangaroo_typical_sampling --task \"safety\" --subtask \"jailbreak\" --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --temperature 0.5 --steps 6  --bench-name \"Kangaroo\" --dtype \"float16\" --do_sample \"top_p\" --max-new-tokens 1024 --hyper_p 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m evaluation.inference_kangaroo_typical_sampling --task \"safety\" --subtask \"jailbreak\" --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --temperature 0.9 --steps 6  --bench-name \"Kangaroo\" --dtype \"float16\" --do_sample \"top_p\" --max-new-tokens 1024 --hyper_p 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipdb\n",
      "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: ipython>=7.31.1 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipdb) (8.18.1)\n",
      "Requirement already satisfied: tomli in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipdb) (2.0.1)\n",
      "Requirement already satisfied: decorator in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipdb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.31.1->ipdb) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from stack-data->ipython>=7.31.1->ipdb) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.16.0)\n",
      "Using cached ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ipdb\n",
      "Successfully installed ipdb-0.13.13\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import shortuuid\n",
    "# Generate answers with local models in a Jupyter Notebook\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from fastchat.utils import str_to_torch_dtype\n",
    "from evaluation.eval import run_eval, reorg_answer_file\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from kangaroo.kangaroo_model import KangarooModel\n",
    "from fastchat.llm_judge.common import load_questions\n",
    "from fastchat.model import get_conversation_template\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kangaroo_forward(inputs, model, tokenizer, max_new_tokens, do_sample=False, max_length=2048, EARLY_STOP_LAYER=2, SPECULATIVE_DECODING_STEPS=6, threshold=0.6):\n",
    "    context_tokens = inputs.input_ids\n",
    "    device = context_tokens.device\n",
    "    token_eos = tokenizer.eos_token_id\n",
    "    batch_size, context_length = context_tokens.shape\n",
    "    global_tokens = torch.ones((batch_size, max_length), dtype=torch.long, device=device) * token_eos\n",
    "    global_position_ids = torch.LongTensor([[i for i in range(max_length)]]).to(device)\n",
    "    accept_length_list = [1]\n",
    "\n",
    "    start_index = context_length\n",
    "    global_tokens[:, :start_index] = context_tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        position_ids = global_position_ids[:, :start_index]\n",
    "        output = model.base_model(context_tokens, position_ids=position_ids, output_hidden_states=True)\n",
    "        model.base_model.past_key_values = list(output.past_key_values)\n",
    "        hidden_state = output.hidden_states[-1]\n",
    "        logits = output.logits\n",
    "        global_tokens[:, start_index] = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "        hidden_state_early = output.hidden_states[EARLY_STOP_LAYER]\n",
    "\n",
    "        hidden_state, adapter_past_key_values = model.adapter_model.forward_early_stop(inputs_embeds=hidden_state_early[:, :, :], position_ids=global_position_ids[:, :context_length], use_cache=True)\n",
    "\n",
    "    total_inference_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_infer_steps = min(max_length, start_index + max_new_tokens)\n",
    "        stop = False\n",
    "\n",
    "        while start_index < max_infer_steps - 1 - SPECULATIVE_DECODING_STEPS:\n",
    "            start_index_copy = start_index\n",
    "            end_index = start_index + 1\n",
    "\n",
    "            for step in range(1 + SPECULATIVE_DECODING_STEPS):\n",
    "                assert adapter_past_key_values[0][0].shape[2] <= end_index - 1, \"{} - {}\".format(adapter_past_key_values[0][0].shape, end_index - 1)\n",
    "                in_tokens_small = global_tokens[:, end_index - 1:end_index]\n",
    "                if adapter_past_key_values[0][0].shape[2] < end_index - 1:\n",
    "                    position_ids = global_position_ids[:, start_index - 1:end_index]\n",
    "                    hidden_state_early_last = exited_hidden_states[:, -1:, :]\n",
    "                else:\n",
    "                    position_ids = global_position_ids[:, end_index - 1:end_index]\n",
    "                    hidden_state_early_last = None\n",
    "\n",
    "                hidden_state_early = model.base_model.forward_draft_or_large_model(in_tokens_small=in_tokens_small[:, -1:], position_ids=position_ids[:, -1:])\n",
    "\n",
    "                if step == 0:\n",
    "                    exited_hidden_states = None\n",
    "\n",
    "                exited_hidden_states = hidden_state_early if exited_hidden_states is None else torch.cat([exited_hidden_states, hidden_state_early], dim=1)\n",
    "\n",
    "                if hidden_state_early_last is not None:\n",
    "                    hidden_state_early = torch.cat([hidden_state_early_last, hidden_state_early], dim=1)\n",
    "\n",
    "                if step == SPECULATIVE_DECODING_STEPS or (step > 0 and predict_score < threshold):\n",
    "                    break\n",
    "\n",
    "                hidden_state, adapter_past_key_values = model.adapter_model.forward_early_stop(inputs_embeds=hidden_state_early, position_ids=position_ids, past_key_values=adapter_past_key_values, use_cache=True)\n",
    "\n",
    "                predict_logits = model.head_model(hidden_state[:, -1:, :]).float()\n",
    "                global_tokens[:, end_index] = torch.argmax(predict_logits[:, -1, :], dim=-1)\n",
    "\n",
    "                end_index += 1\n",
    "                predict_score = predict_logits.softmax(dim=-1).max().item()\n",
    "\n",
    "            position_ids = global_position_ids[:, start_index:end_index]\n",
    "            assert model.base_model.past_key_values[EARLY_STOP_LAYER][0].shape[2] == start_index, \"{} - {}\".format(model.base_model.past_key_values[EARLY_STOP_LAYER][0].shape, start_index)\n",
    "            assert exited_hidden_states.shape[1] == position_ids.shape[1]\n",
    "            hidden_state_, hidden_state = model.base_model.forward_draft_or_large_model(in_features_large=exited_hidden_states, position_ids=position_ids)\n",
    "\n",
    "            logits = model.head_model(hidden_state).float()\n",
    "            output_tokens = torch.argmax(logits[:, :, :], dim=-1)\n",
    "\n",
    "            output_length = end_index - start_index\n",
    "            for i in range(output_length):\n",
    "                if i == output_length - 1 or output_tokens[0, i] == token_eos or output_tokens[0, i] != global_tokens[0, start_index + 1 + i]:\n",
    "                    global_tokens[0, start_index + 1 + i] = output_tokens[0, i]\n",
    "                    start_index = start_index + 1 + i\n",
    "                    if output_tokens[0, i] == token_eos:\n",
    "                        stop = True\n",
    "                    break\n",
    "\n",
    "            accept_length_list.append(start_index - start_index_copy)\n",
    "            hidden_state = hidden_state[:, :output_length - (end_index - start_index), :]\n",
    "\n",
    "            if model.base_model.past_key_values[0][0].shape[2] > start_index:\n",
    "                past_key_values_large_ = []\n",
    "                for k, v in model.base_model.past_key_values:\n",
    "                    past_key_values_large_.append((k[:, :, :start_index, :], v[:, :, :start_index, :]))\n",
    "                model.base_model.past_key_values = past_key_values_large_\n",
    "\n",
    "            if adapter_past_key_values[0][0].shape[2] > start_index:\n",
    "                adapter_past_key_values_ = []\n",
    "                for k, v in adapter_past_key_values:\n",
    "                    adapter_past_key_values_.append((k[:, :, :start_index, :], v[:, :, :start_index, :]))\n",
    "                adapter_past_key_values = tuple(adapter_past_key_values_)\n",
    "                del adapter_past_key_values_\n",
    "\n",
    "            total_inference_steps += 1\n",
    "\n",
    "            if stop:\n",
    "                break\n",
    "\n",
    "    output_ids = global_tokens[0, :start_index + 1].tolist()\n",
    "    new_token = start_index - context_length + 1\n",
    "    idx = len(accept_length_list) - 1\n",
    "    return [output_ids], new_token, idx, accept_length_list\n",
    "\n",
    "# Parameters for the notebook\n",
    "class Args:\n",
    "    model_path = \"vicuna-7b-v1.3\"\n",
    "    adapter_path = \"kangaroo-vicuna-7b-v1.3\"\n",
    "    model_id = \"vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16\"\n",
    "    bench_name = \"Kangaroo\"\n",
    "    question_begin = None\n",
    "    question_end = None\n",
    "    answer_file = None\n",
    "    max_new_tokens = 1024\n",
    "    num_choices = 1\n",
    "    num_gpus_per_model = 1\n",
    "    num_gpus_total = 1\n",
    "    threshold = 0.6\n",
    "    exitlayer = 2\n",
    "    steps = 6\n",
    "    dtype = \"float16\"\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Run the evaluation\n",
    "question_file = f\"data/question.jsonl\"\n",
    "\n",
    "os.makedirs(f\"data/{args.bench_name}/{args.model_id}\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae84299f3e948eeb9454625ed341834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model = KangarooModel(args.model_path, args.adapter_path, args, EARLY_STOP_LAYER=args.exitlayer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "do_sample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16\n"
     ]
    }
   ],
   "source": [
    "print(f\"data/{args.bench_name}/{args.model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     answer_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mbench_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     run_eval(\n\u001b[1;32m      6\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      7\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         EARLY_STOP_LAYER\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mexitlayer\n\u001b[1;32m     22\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    answer_file = f\"data/{args.bench_name}/{args.model_id}/{run}.jsonl\"\n",
    "    print(f\"Output to {answer_file}\")\n",
    "\n",
    "    run_eval(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        forward_func=kangaroo_forward,\n",
    "        model_id=args.model_id,\n",
    "        question_file=question_file,\n",
    "        question_begin=args.question_begin,\n",
    "        question_end=args.question_end,\n",
    "        answer_file=answer_file,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        num_choices=args.num_choices,\n",
    "        num_gpus_per_model=args.num_gpus_per_model,\n",
    "        num_gpus_total=args.num_gpus_total,\n",
    "        do_sample=do_sample,\n",
    "        threshold=args.threshold,\n",
    "        SPECULATIVE_DECODING_STEPS=args.steps,\n",
    "        EARLY_STOP_LAYER=args.exitlayer\n",
    "    )\n",
    "\n",
    "    reorg_answer_file(answer_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.58s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/0.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 1\n",
      "Warmup done\n",
      " 49%|████████████████████▏                    | 236/480 [27:46<22:00,  5.41s/it]ERROR question ID:  317\n",
      "100%|█████████████████████████████████████████| 480/480 [44:32<00:00,  5.57s/it]\n",
      "#Mean accepted tokens:  2.050384038336655\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/1.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 1\n",
      "^C\n",
      "ERROR when forwarding ERROR ID:  81\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/inference_kangaroo_safety.py\", line 225, in <module>\n",
      "    run_eval(\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/eval_safety.py\", line 54, in run_eval\n",
      "    get_answers_func(\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/eval_safety.py\", line 123, in get_model_answers\n",
      "UnboundLocalError: local variable 'output_ids' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m evaluation.inference_kangaroo_safety --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --steps 6 --model-id \"vicuna-7b-v1.3-kangaroo-test\" --bench-name \"Kangaroo\" --dtype \"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            accept_length = apply_typical_acceptance(logits, temperature=1.0, posterior_threshold=0.1, posterior_alpha=1.0, fast=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kangaroo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
