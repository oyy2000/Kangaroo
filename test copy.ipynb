{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.81s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/0.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [44:11<00:00,  5.52s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/1.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [44:10<00:00,  5.52s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/2.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [44:11<00:00,  5.52s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python -m evaluation.inference_kangaroo --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --steps 6 --model-id \"vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16\" --bench-name \"Kangaroo\" --dtype \"float16\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.86s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/0.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [43:45<00:00,  5.47s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/1.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [43:29<00:00,  5.44s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/2.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [43:48<00:00,  5.48s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python -m evaluation.inference_kangaroo --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --steps 6 --model-id \"vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16\" --bench-name \"Kangaroo\" --dtype \"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.73s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/0.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "Warmup done\n",
      "100%|█████████████████████████████████████████| 480/480 [46:22<00:00,  5.80s/it]\n",
      "#Mean accepted tokens:  2.0496360469789665\n",
      "Output to data/Kangaroo/vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16/1.jsonl\n",
      "Check model training state: False\n",
      "CUDA VISIBLE DEVICES: 2\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/inference_kangaroo.py\", line 225, in <module>\n",
      "    run_eval(\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/eval.py\", line 54, in run_eval\n",
      "    get_answers_func(\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/eval.py\", line 110, in get_model_answers\n",
      "    output_ids, new_token, idx, accept_length_tree = forward_func(\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/inference_kangaroo.py\", line 89, in kangaroo_forward\n",
      "    hidden_state_, hidden_state = model.base_model.forward_draft_or_large_model(in_features_large=exited_hidden_states, position_ids=position_ids)\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/kangaroo/earlyexit.py\", line 57, in forward_draft_or_large_model\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 424, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/yo46/anaconda3/envs/kangaroo/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 337, in forward\n",
      "    key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python -m evaluation.inference_kangaroo --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --steps 6 --model-id \"vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16\" --bench-name \"Kangaroo\" --dtype \"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yo46/learning/decoding/TrustLLM/Kangaroo/evaluation/inference_kangaroo.py\", line 11, in <module>\n",
      "    from evaluation.eval import run_eval, reorg_answer_file\n",
      "ModuleNotFoundError: No module named 'evaluation'\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python -m evaluation.inference_kangaroo --adapter-path \"kangaroo-vicuna-7b-v1.3\" --exitlayer 2 --model-path \"vicuna-7b-v1.3\" --threshold 0.6 --steps 6 --model-id \"vicuna-7b-v1.3-kangaroo-thres-0.6-steps-6-float16\" --bench-name \"Kangaroo\" --dtype \"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kangaroo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
